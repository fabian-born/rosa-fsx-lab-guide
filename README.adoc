= ROSA FSx Workshop
:numbered:

This lab guide provides a walkthrough on using FSx for ONTAP as a persistent storage layer for applications on *Red Hat OpenShift Service on AWS* (ROSA). It will guide you through the steps to install the NetApp Trident Container Storage Interface (CSI) driver on a ROSA cluster. Additionally, you will learn how to provision an FSx for ONTAP file system, deploy a sample stateful application. To ensure your dataâ€™s resilience, you learn how to backup and restore your application data. 


Red Hat associates that have access to the Red Hat Demo Platform (RHDP) can launch the ROSA FSx Workshop for a lab environment.

Note: This lab guide assumes you are using the RHDP environment.   


Once you have the lab environment ready,  SSH into the bastion and follow the steps below.

Steps: 

. <<lab-setup, Lab Setup>>
. <<provision-fsx-for-ontap, Provision FSx for ONTAP>>
. <<install-and-configure-the-trident-csi-driver-for-rosa, Install and Configure the Trident CSI driver for ROSA>>
. << Install and Configure the Trident CSI backend to FSx for ONTAP
. <<deploy-sample-mysql-stateful-application, Deploy a Sample MySQL Stateful Application>>
. <<backup-and-restore-with-snapshots, Backup and Restore with Snapshots>>



[[lab-setup]]
== Lab Setup

=== Login oc command to the cluster

The bastion provided with the RHDP workshop has the `aws` and `rosa` command line tools already installed and logged in.  The `oc` command is installed, but not logged in.



To login `oc` use the API URL, username, and password provided by the RHDP lab deployment.

Replace the password with your own. Remember this is senstive data, so do not share with anyone.

[source,bash]
----
oc login https://api.rosa-abc12.ab12.p1.openshiftapps.com:6443 -u cluster-admin -p {{ password }}
----

=== Clone git

Clone the git repository:

[source,bash]
----
git clone https://github.com/redhat-gpst/rosa-fsx-lab-guide.git rosa-fsx
----

`cd` into the rosa-fsx/fsx directory. This will be the main working directory for the lab.

[source,shell]
----
cd ~/rosa-fsx/fsx
----


== Provision FSx for ONTAP  [[provision-fsx]]


* FSx for NetApp ONTAP provides fully managed shared storage with features like snapshots, cloning, and data tiering. It's integrated with AWS, allowing for seamless cloud storage solutions.
* In this step, you're setting up the FSx for ONTAP file system which will be used as the backend storage for your OpenShift applications.
* We will create a single-AZ FSx for ONTAP file system in the same VPC as the ROSA cluster.

=== Get Subnet and VPC IDs

Let's take a look at the VPC subnets.

From the bastion, run the following command.  

[source,bash]
----
aws ec2 describe-subnets --query 'Subnets[*].{SubnetId:SubnetId,VpcId:VpcId,CidrBlock:CidrBlock}' \
   --output table
----

Output from commands will be showin in a box like below.

[listing]
----
-------------------------------------------------------------------------
|                             DescribeSubnets                           |
+----------------+----------------------------+-------------------------+
|    CidrBlock   |         SubnetId           |          VpcId          |
+----------------+----------------------------+-------------------------+
|  10.0.0.0/17   |  subnet-0c1e3b083f692a17f  |  vpc-0994809fd6f55252b  |
|  192.168.0.0/24|  subnet-01b8fa59d97657eca  |  vpc-0a4106cf5b3b895b5  |
|  10.0.128.0/17 |  subnet-0299fe13ba470aa9f  |  vpc-0994809fd6f55252b  |
+----------------+----------------------------+-------------------------+
----

Run the following command to assign the SubnetId to the SUBNETID variable.

[source,bash]
----
export SUBNETID=$(aws ec2 describe-subnets --query 'Subnets[?CidrBlock==`10.0.0.0/17`].SubnetId' \
   --output json | jq -r '.[0]') && echo $SUBNETID
----

Run the following command to assign the VpcId to the VPCID variable.

[source,bash]
----
export VPCID=$(aws ec2 describe-subnets --query 'Subnets[?CidrBlock==`10.0.0.0/17`].VpcId' \
   --output json | jq -r '.[0]') && echo $VPCID
----

=== Create the FSx stack

Run the following command to create the stack

[source,shell]
----
aws cloudformation create-stack \
  --stack-name ROSA-FSXONTAP \
  --template-body file://./FSxONTAP.yml \
  --region us-east-2 \
  --parameters \   
     ParameterKey=Subnet1ID,ParameterValue=$SUBNETID \
     ParameterKey=myVpc,ParameterValue=$VPCID \
     ParameterKey=FileSystemName,ParameterValue=ROSA-myFSxONTAP \
     ParameterKey=ThroughputCapacity,ParameterValue=512 \
     ParameterKey=FSxAllowedCIDR,ParameterValue=10.0.0.0/16 \
     ParameterKey=FsxAdminPassword,ParameterValue=Rosa12345 \
     ParameterKey=SvmAdminPassword,ParameterValue=Rosa12345 \
  --capabilities CAPABILITY_NAMED_IAM
----

NOTE: This can take 20 - 30 minutes

You can monitor the progress with the following command. You may have to run it a few times before the stack is fully configured.

[source,bash]
----
aws cloudformation describe-stacks --stack ROSA-FSXONTAP | grep 'StackStatus'
----
[listing]
----
  "StackStatus": "CREATE_IN_PROGRESS",
----

Once the stack is ready, you will se the `CREATE_COMPLETE` status
[listing]
----
"StackStatus": "CREATE_COMPLETE",
----

Verify your file system and storage virtual machine (SVM1) have been created.

[source,bash]
----
aws fsx describe-file-systems
----

[source,bash]
----
aws fsx describe-storage-virtual-machines
----

== Install and Configure the Trident CSI driver for ROSA [[config-trident]]

* Trident is NetApp's dynamic storage orchestrator for OpenShift. It automates and manages storage resources for containers.
* By installing Trident, you're enabling your ROSA cluster to dynamically provision and manage storage resources on FSx for ONTAP, providing a robust and scalable storage solution for your applications.

=== Install Trident

To begin, add the Astra Trident Helm repository

[source,bash]
----
helm repo add netapp-trident https://netapp.github.io/trident-helm-chart
----

Use `helm install` to install the Trident driver in the `trident` namespace. You may see a warning about Pod Security. It can be ignored.

[source,bash]
----
helm install trident netapp-trident/trident-operator --version 23.01.1 --create-namespace --namespace trident
----

Run the following command to verify the Trident driver installation.

[source,shell]
----
helm status trident -n trident | grep "NAME:" -A 5
----
[listing]
----
NAME: trident
LAST DEPLOYED: Mon Nov  6 20:52:31 2023
NAMESPACE: trident
STATUS: deployed
REVISION: 1
TEST SUITE: None
----


=== Create a secret to store the SVM username and password in the ROSA cluster

View the `svm_secret.yml` file. Take note of the password.
[source,bash]
----
cat svm_secret.yml
----
[listing]
----
apiVersion: v1
kind: Secret
metadata:
  name: backend-fsx-ontap-nas-secret
  namespace: trident
type: Opaque
stringData:
  username: vsadmin
  password: Rosa12345
----

Add the secret to the ROSA cluster with the following command:

[source, bash]
----
oc apply -f svm_secret.yml
----

To verify the secret has been added to the ROSA cluster, run the following command.

[source,bash]
----
oc get secrets -n trident | awk '/NAME|backend-fsx-ontap-nas-secret/'
----
[listing]
----
NAME                                 TYPE                                  DATA   AGE
backend-fsx-ontap-nas-secret         Opaque                                2      24h
----


== Install and Configure the Trident CSI backend to FSx for ONTAP

* The Trident backend configuration tells Trident how to communicate with the storage system, in this case, FSx for ONTAP. 
* We willl use the `ontap-nas` driver to provision storage volumes.
* We are going to edit `backend-ontap-nas.yml` so it has the IP from the ManagementLIF and DataLIF IP addresses of the FSx Server Virtual Mancine.


=== Assign the Management IP and SVM IP variables.

[source,bash]
----
export SVMIP=$(aws fsx describe-storage-virtual-machines | jq -r '.StorageVirtualMachines[].Endpoints.Management.IpAddresses[]') && echo $SVMIP
----

And then update `backend-ontap-nas.yml`

[source,bash]
----
sed -i "s/<<management-ip>>/$SVMIP/g" backend-ontap-nas.yml
----

Review the contents of the file:

[source,bash]
----
cat backend-ontap-nas.yml
----

Example:
[listing]
----
apiVersion: trident.netapp.io/v1
kind: TridentBackendConfig
metadata:
  name: backend-fsx-ontap-nas
  namespace: trident
spec:
  version: 1
  backendName: fsx-ontap
  storageDriverName: ontap-nas
  managementLIF: 10.0.50.139
  dataLIF: 10.0.50.139
  svm: SVM1
  credentials:
    name: backend-fsx-ontap-nas-secret
----

Next execute the following commands to configure the Trident backend in the ROSA cluster.

[source,shell]
----
oc apply -f backend-ontap-nas.yml
----

Verify the backend configuration.

[source,shell]
----
oc get tbc -n trident
----
[listing]
----
NAME                    BACKEND NAME   BACKEND UUID                           PHASE   STATUS
backend-fsx-ontap-nas   fsx-ontap      1f490bf3-492c-4ef7-899e-9e7d8711c82f   Bound   Success
----

== Create storage class in ROSA cluster

* A storage class defines how storage is dynamically provisioned, specifying attributes like size and performance.
* A storage class automates the creation of storage volumes when applications request storage through PVCs.
* This configures a storage class to work with Trident, ensuring efficient management of FSx for NetApp ONTAP as backend storage.

=== Create the new `trident-csi` storage class.  

Note: Use `cat` to view any files before applying them.

[source,shell]
----
oc apply -f storage-class-csi-nas.yml
----

Verify the status of the trident-csi storage class creation.

[source,shell]
----
oc get sc | awk '/NAME|trident-csi/'
----
[listing]
----
NAME            PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
trident-csi     csi.trident.netapp.io   Retain          Immediate              true                   1h58m
----

== Deploy MySQL stateful application [[deploy-mysql]]

* Stateful applications, like databases, need to maintain data across pod restarts. 
* Using PVCs, Trident ensures data persistence for these applications.
* Deploying MySQL as a sample stateful application gives a practical example of how Trident integrates with OpenShift to manage data for stateful applications, ensuring data is not lost when pods are moved or restarted.

=== Setup the MySQL Project

Before we create the MySQL application, we will creat a `mysql` project and store the applicationâ€™s username and password in a Secret. 

Create the mysql namespace
[source,bash]
----
oc create ns mysql
----

We'll use the `mysql` project as our default project
[source,bash]
----
oc project mysql
----

Create the mysql secret

Note: `password` is the password but can be chagned in the `mysql-secret.yml` file

[source,bash]
----
oc apply -f mysql-secret.yml
----

Now, verify the secrets were created.

[source,bash]
----
oc get secrets | awk '/NAME|mysql-password/'
----
[listing]
----
NAME                       TYPE                                  DATA   AGE
mysql-password             opaque                                1      1h34m
----

=== Create a pvc for the MySQL application

[source,bash]
----
oc apply -f mysql-pvc.yml
----


Verify the PVCs are created by the MySQL application. 

[source,shell]
----
oc get pvc
----
[listing]
----
NAME          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mysql-volume  Bound    pvc-676d059c-1480-4e36-963e-2249efc70331   10Gi       RWX            trident-csi    4h4m
----

=== MySQL Application Deployment

Next we will deploy the MySQL application on the ROSA cluster. 

Open `mysql-deployment.yml` and review the details â€“metadata, replicas, and storageclass name.
For simplicity in this lab, we are only going to create one (1) replica set.


Execute the following command.  

NOTE: Ignore any warnings about PodSecurity

[source,shell]
----
oc apply -f mysql-deployment.yml
----

Verify the application deployment.  It will take a minute for the container to start.

[source,shell]
----
oc get pods
----

[listing]
----
NAME                        READY   STATUS    RESTARTS   AGE
mysql-fsx-7db4f45b8-mmfzv   1/1     Running   0          40s

----

=== Create a service for the MySQL application

* A service in OpenShift acts as an internal load balancer. It provides a stable endpoint through which other pods within the cluster can access the MySQL application, regardless of the individual states of the MySQL pods.
* By specifying a service for MySQL, you provide a consistent internal address for the database, ensuring seamless communication even as pods are scaled or restarted.

[source,shell]
----
oc apply -f mysql-service.yml
----

Verify the service.

[source,shell]
----
oc get svc
----
[listing]
----
NAME    TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
mysql   ClusterIP   None         <none>        3306/TCP   4h3m
----

=== Create MySQL client for MySQL

* The MySQL client is used to access the MySQL application using the service we created.
* This provides a consistent entry point into the database.

Review the content of `mysql-client.yml` and then deploy the MySQL client using the following command.

[source,shell]
----
oc apply -f mysql-client.yml
----

Verify the pod status.

[source,shell]
----
oc get pods
---- 

=== Create a sample database

Log in to the MySQL client pod.

[source,shell]
----
oc exec --stdin --tty mysql-client -- sh
----

Then, Install the MySQL client tool.

[source,shell]
----
apk add mysql-client
----

Within the `mysql-client` pod, connect to the MySQL server.

[source,shell]
----
mysql -u root -p -h mysql-set-0.mysql.mysql.svc.cluster.local
----

Enter the password that is stored in mysql-secrets.yml. Once connected, Create a database on the MySQL database.

From the `MySQL [(none)]>` prompt enter the following:

[source]
----
CREATE DATABASE erp;
CREATE TABLE erp.Persons ( ID int, FirstName varchar(255),Lastname varchar(255)); 
INSERT INTO erp.Persons (ID, FirstName, LastName) values (1234 , "John" , "Doe");
commit;
select * from erp.Persons;
----

[listing]
----
+------+-----------+----------+
| ID | FirstName | Lastname |
+------+-----------+----------+
| 1234 | John | Doe |
+------+-----------+----------+
----

Type `exit` to exit the mysql server and `exit` again to exit the pod.  You should now be back at the bastion prompt

== Creating Snapshots [[creating-snapshots]]

* Snapshots are point-in-time copies of your data, crucial for backup and disaster recovery.
* Here, youâ€™ll learn how to use Trident with FSx for ONTAP to create snapshots for backup, and how to restore your application data from these snapshots. 
* This is vital for protecting your application against data loss.


=== Create the volume snapshot class and snapshot

[source,bash]
----
oc apply -f volume-snapshot-class.yml
----

Next, create a snapshot of the exising MySQl data

[source,bash]
----
oc apply -f volume-snapshot.yml
----

Use the following to find the name of the snapshot.

[source, bash]
----
oc get volumesnapshots
----

== Data Recovery

* This part of the lab illustrates the use of snapshots in real-world scenarios through the deletion and restoration of the database.
* This demonstrates the quick and efficient data recovery capabilities of Trident and FSx for ONTAP in managing and protecting OpenShift stateful application data. 

=== Delete the `erp` database

To delete the database "erp" after creating a snapshot follow these steps:

[source,bash]
----
oc exec --stdin --tty mysql-client -n mysql -- sh
----

Login to the MYSQL database.

[source,bash]
----
mysql -u root -p -h mysql.mysql.svc.cluster.local
----




Delete the "erp" database at the `MySQL [(none)]>` prompt

[source,sql]
----
DROP DATABASE erp;
----

After executing the DROP command, the database "erp" will be deleted, and you should see a message like:

[listing]
----
Query OK, 1 row affected
----

=== Restore the snapshot

First, create a new pvc from the snapshot.  

Note the name of the new pvc `mysql-volume-clone`

[source,bash]
----
oc apply -f mysql-pvc-clone.yml
----

=== Update the MySQL application

We need to to update the `mysql` application to point to the new pvc.

Edit `mysql-deployment.yml` with your favorite editor, `vim`

Update the last line with the name of the pvc we just created, `mysql-volume-clone`

[source]
----
  claimName: mysql-volume-clone 
----

Redeploy the application.  This will recreate the pod so it points to the cloned pvc.
[source,bash]
----
oc apply -f mysql-deployment.yml
----

Verify the new pod is running.  This may take a minute.

[source,bash]
----
oc get pods -n mysql
----

=== Validate the Database Restored

* Validation confirms that the restored data is complete and accurate, maintaining the integrity of the database after a recovery process.
* Validation helps in identifying any issues or gaps in the restoration process, allowing for timely corrections


We can now check that our data has been restored.


[source,bash]
----
oc exec --stdin --tty mysql-client -n mysql -- sh
----
[source,bash]
----
mysql -u root -p -h mysql.mysql.svc.cluster.local
----

Show Databases

[source,sql]
----
MySQL [(none)]> SHOW DATABASES;
+--------------------+
| Database           |
+--------------------+
| erp                |
+--------------------+
----

Show database data

[source,sql]
----
MySQL [(none)]> select * from erp.Persons;
+------+-----------+----------+
| ID   | FirstName | Lastname |
+------+-----------+----------+
| 1234 | John      | Doe      |
+------+-----------+----------+
----


Congrats.  You have completed the lab!